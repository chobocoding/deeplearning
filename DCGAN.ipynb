{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/DeVyacheslav/PyTorch-MNIST-DCGAN/blob/master/MNIST_DCGAN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, channels_img, features_d):\n",
    "    super(Discriminator, self).__init__()\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        # N x channels_img x 64 x 64\n",
    "        nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(features_d),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        # N x features_d x 32 x 32\n",
    "        nn.Conv2d(features_d, features_d*2, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(features_d*2),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        # N x features_d*2 x 16 x 16\n",
    "        nn.Conv2d(features_d*2, features_d*4, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(features_d*4),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        # N x features_d*4 x 8 x 8\n",
    "        nn.Conv2d(features_d*4, features_d*8, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(features_d*8),\n",
    "        nn.LeakyReLU(0.2),\n",
    "\n",
    "        # N x features_d*8 x 4 x 4\n",
    "        nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self, channels_noise, channels_img, features_g):\n",
    "    super(Generator, self).__init__()\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        # N x channels_noise x 1 x 1\n",
    "        nn.ConvTranspose2d(channels_noise, features_g*16, kernel_size=4, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(features_g*16),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        # N x features_g*16 x 4 x 4\n",
    "        nn.ConvTranspose2d(features_g*16, features_g*8, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(features_g*8),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        # N x features*8 x 8 x 8\n",
    "        nn.ConvTranspose2d(features_g*8, features_g*4, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(features_g*4),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        # N x features*4 x 16 x 16\n",
    "        nn.ConvTranspose2d(features_g*4, features_g*2, kernel_size=4, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(features_g*2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        # N x features*2 x 32 x 32\n",
    "        nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_D = 0.0002\n",
    "lr_G = 0.0002\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "channels_img = 3\n",
    "channels_noise = 256\n",
    "num_epochs = 100\n",
    "\n",
    "features_d = 32\n",
    "features_g = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transforms = transforms.Compose([\n",
    "  transforms.Resize(image_size),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='dataset/', train=True, transform=custom_transforms, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "  \n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_D = Discriminator(channels_img, features_d).to(device)\n",
    "net_G = Generator(channels_noise, channels_img, features_g).to(device)\n",
    "\n",
    "optimizator_D = optim.Adam(net_D.parameters(), lr=lr_D)\n",
    "optimizator_G = optim.Adam(net_G.parameters(), lr=lr_G)\n",
    "\n",
    "net_D.train()\n",
    "net_G.train()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "fixed_noise = torch.randn(64, channels_noise, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Start training\n",
      "Epoch 0/100, Batch 781/782   Loss D 0.0527, Loss G: 0.7492, D_mean: 0.7156\n",
      "Epoch 1/100, Batch 781/782   Loss D 0.0744, Loss G: 0.6612, D_mean: 0.8415\n",
      "Epoch 2/100, Batch 781/782   Loss D 0.0531, Loss G: 0.5952, D_mean: 0.7488\n",
      "Epoch 3/100, Batch 781/782   Loss D 0.0346, Loss G: 1.0665, D_mean: 0.8403\n",
      "Epoch 4/100, Batch 781/782   Loss D 0.0236, Loss G: 0.8746, D_mean: 0.8157\n",
      "Epoch 5/100, Batch 781/782   Loss D 0.0073, Loss G: 0.8400, D_mean: 0.9308\n",
      "Epoch 6/100, Batch 781/782   Loss D 0.0666, Loss G: 0.5909, D_mean: 0.7573\n",
      "Epoch 7/100, Batch 781/782   Loss D 0.1072, Loss G: 0.6963, D_mean: 0.6719\n",
      "Epoch 8/100, Batch 781/782   Loss D 0.0616, Loss G: 0.9452, D_mean: 0.8488\n",
      "Epoch 9/100, Batch 781/782   Loss D 0.0735, Loss G: 0.7592, D_mean: 0.7215\n",
      "Epoch 10/100, Batch 781/782   Loss D 0.0833, Loss G: 0.6415, D_mean: 0.8255\n",
      "Epoch 11/100, Batch 781/782   Loss D 0.2369, Loss G: 0.8108, D_mean: 0.4955\n",
      "Epoch 12/100, Batch 781/782   Loss D 0.0986, Loss G: 0.7694, D_mean: 0.9154\n",
      "Epoch 13/100, Batch 781/782   Loss D 0.0654, Loss G: 1.1471, D_mean: 0.8030\n",
      "Epoch 14/100, Batch 781/782   Loss D 0.1287, Loss G: 0.6555, D_mean: 0.5841\n",
      "Epoch 15/100, Batch 781/782   Loss D 0.1027, Loss G: 0.7568, D_mean: 0.6525\n",
      "Epoch 16/100, Batch 781/782   Loss D 0.0296, Loss G: 0.8459, D_mean: 0.9176\n",
      "Epoch 17/100, Batch 781/782   Loss D 0.0613, Loss G: 1.4297, D_mean: 0.9265\n",
      "Epoch 18/100, Batch 781/782   Loss D 0.0695, Loss G: 0.8972, D_mean: 0.7121\n",
      "Epoch 19/100, Batch 781/782   Loss D 0.0241, Loss G: 0.6980, D_mean: 0.8795\n",
      "Epoch 20/100, Batch 781/782   Loss D 0.0438, Loss G: 0.9538, D_mean: 0.7972\n",
      "Epoch 21/100, Batch 781/782   Loss D 0.0397, Loss G: 0.8580, D_mean: 0.7859\n",
      "Epoch 22/100, Batch 781/782   Loss D 0.0504, Loss G: 0.7686, D_mean: 0.7905\n",
      "Epoch 23/100, Batch 781/782   Loss D 0.1103, Loss G: 0.4663, D_mean: 0.9338\n",
      "Epoch 24/100, Batch 781/782   Loss D 0.0822, Loss G: 0.7061, D_mean: 0.6947\n",
      "Epoch 25/100, Batch 781/782   Loss D 0.1531, Loss G: 0.5444, D_mean: 0.5493\n",
      "Epoch 26/100, Batch 781/782   Loss D 0.1061, Loss G: 0.7576, D_mean: 0.9040\n",
      "Epoch 27/100, Batch 781/782   Loss D 0.0511, Loss G: 0.7309, D_mean: 0.8844\n",
      "Epoch 28/100, Batch 781/782   Loss D 0.0393, Loss G: 0.9899, D_mean: 0.9998\n",
      "Epoch 29/100, Batch 781/782   Loss D 0.2522, Loss G: 0.6933, D_mean: 1.2052\n",
      "Epoch 30/100, Batch 781/782   Loss D 0.0348, Loss G: 0.9567, D_mean: 0.8089\n",
      "Epoch 31/100, Batch 781/782   Loss D 0.1634, Loss G: 0.7251, D_mean: 1.0503\n",
      "Epoch 32/100, Batch 781/782   Loss D 0.0797, Loss G: 0.6407, D_mean: 0.6958\n",
      "Epoch 33/100, Batch 781/782   Loss D 0.0718, Loss G: 0.9172, D_mean: 0.7344\n",
      "Epoch 34/100, Batch 781/782   Loss D 0.1486, Loss G: 0.7081, D_mean: 0.5499\n",
      "Epoch 35/100, Batch 781/782   Loss D 0.1216, Loss G: 0.5633, D_mean: 0.6742\n",
      "Epoch 36/100, Batch 781/782   Loss D 0.4122, Loss G: 0.4222, D_mean: 0.2817\n",
      "Epoch 37/100, Batch 781/782   Loss D 0.2284, Loss G: 0.4799, D_mean: 0.9185\n",
      "Epoch 38/100, Batch 781/782   Loss D 0.1969, Loss G: 0.9653, D_mean: 0.4982\n",
      "Epoch 39/100, Batch 781/782   Loss D 0.1458, Loss G: 0.5322, D_mean: 0.7243\n",
      "Epoch 40/100, Batch 781/782   Loss D 0.0328, Loss G: 0.8018, D_mean: 0.8551\n",
      "Epoch 41/100, Batch 781/782   Loss D 0.1404, Loss G: 0.7251, D_mean: 0.7596\n",
      "Epoch 42/100, Batch 781/782   Loss D 0.3046, Loss G: 1.0728, D_mean: 0.4146\n",
      "Epoch 43/100, Batch 781/782   Loss D 0.0953, Loss G: 0.5827, D_mean: 0.6664\n",
      "Epoch 44/100, Batch 781/782   Loss D 0.2713, Loss G: 0.4010, D_mean: 0.8025\n",
      "Epoch 45/100, Batch 781/782   Loss D 0.1215, Loss G: 0.5458, D_mean: 0.6267\n",
      "Epoch 46/100, Batch 781/782   Loss D 0.0875, Loss G: 0.9044, D_mean: 0.9052\n",
      "Epoch 47/100, Batch 781/782   Loss D 0.0360, Loss G: 1.0572, D_mean: 0.8323\n",
      "Epoch 48/100, Batch 781/782   Loss D 0.0477, Loss G: 0.7256, D_mean: 0.8186\n",
      "Epoch 49/100, Batch 781/782   Loss D 0.0587, Loss G: 0.7659, D_mean: 0.9181\n",
      "Epoch 50/100, Batch 781/782   Loss D 0.0620, Loss G: 1.0583, D_mean: 0.7343\n",
      "Epoch 51/100, Batch 781/782   Loss D 0.1354, Loss G: 0.5942, D_mean: 0.6185\n",
      "Epoch 52/100, Batch 781/782   Loss D 0.0651, Loss G: 0.7055, D_mean: 0.7057\n",
      "Epoch 53/100, Batch 781/782   Loss D 0.0582, Loss G: 0.9209, D_mean: 0.7343\n",
      "Epoch 54/100, Batch 781/782   Loss D 0.1016, Loss G: 0.4653, D_mean: 0.6952\n",
      "Epoch 55/100, Batch 781/782   Loss D 0.0370, Loss G: 0.7946, D_mean: 0.9313\n",
      "Epoch 56/100, Batch 781/782   Loss D 0.1825, Loss G: 0.5807, D_mean: 0.5210\n",
      "Epoch 57/100, Batch 781/782   Loss D 0.1322, Loss G: 0.7608, D_mean: 0.9247\n",
      "Epoch 58/100, Batch 781/782   Loss D 0.0812, Loss G: 0.7660, D_mean: 0.7033\n",
      "Epoch 59/100, Batch 781/782   Loss D 0.0791, Loss G: 0.7525, D_mean: 0.8973\n",
      "Epoch 60/100, Batch 781/782   Loss D 0.0591, Loss G: 0.6786, D_mean: 0.7164\n",
      "Epoch 61/100, Batch 781/782   Loss D 0.0909, Loss G: 0.6995, D_mean: 0.7836\n",
      "Epoch 62/100, Batch 781/782   Loss D 0.3183, Loss G: 0.6146, D_mean: 0.3720\n",
      "Epoch 63/100, Batch 781/782   Loss D 0.0446, Loss G: 0.8117, D_mean: 0.7886\n",
      "Epoch 64/100, Batch 781/782   Loss D 0.2488, Loss G: 0.3751, D_mean: 0.4752\n",
      "Epoch 65/100, Batch 781/782   Loss D 0.1429, Loss G: 0.6804, D_mean: 0.6125\n",
      "Epoch 66/100, Batch 781/782   Loss D 0.2141, Loss G: 0.5525, D_mean: 0.4865\n",
      "Epoch 67/100, Batch 781/782   Loss D 0.0518, Loss G: 0.9811, D_mean: 0.7207\n",
      "Epoch 68/100, Batch 781/782   Loss D 0.0335, Loss G: 0.8797, D_mean: 0.8951\n",
      "Epoch 69/100, Batch 781/782   Loss D 0.0686, Loss G: 0.5825, D_mean: 0.7923\n",
      "Epoch 70/100, Batch 781/782   Loss D 0.0991, Loss G: 0.7645, D_mean: 0.6810\n",
      "Epoch 71/100, Batch 781/782   Loss D 0.0374, Loss G: 0.6599, D_mean: 0.8214\n",
      "Epoch 72/100, Batch 781/782   Loss D 0.2926, Loss G: 0.4905, D_mean: 0.4107\n",
      "Epoch 73/100, Batch 781/782   Loss D 0.0984, Loss G: 1.5209, D_mean: 1.1073\n",
      "Epoch 74/100, Batch 781/782   Loss D 0.0445, Loss G: 1.0550, D_mean: 0.9724\n",
      "Epoch 75/100, Batch 781/782   Loss D 0.0322, Loss G: 0.8285, D_mean: 0.8057\n",
      "Epoch 76/100, Batch 781/782   Loss D 0.0667, Loss G: 0.9079, D_mean: 0.6938\n",
      "Epoch 77/100, Batch 781/782   Loss D 0.0218, Loss G: 0.8186, D_mean: 0.8809\n",
      "Epoch 78/100, Batch 781/782   Loss D 0.0963, Loss G: 0.7053, D_mean: 0.7040\n",
      "Epoch 79/100, Batch 781/782   Loss D 0.0769, Loss G: 0.5590, D_mean: 0.7005\n",
      "Epoch 80/100, Batch 781/782   Loss D 0.0363, Loss G: 0.9009, D_mean: 0.8129\n",
      "Epoch 81/100, Batch 781/782   Loss D 0.2724, Loss G: 0.4909, D_mean: 0.4133\n",
      "Epoch 82/100, Batch 781/782   Loss D 0.0694, Loss G: 0.6255, D_mean: 0.6824\n",
      "Epoch 83/100, Batch 781/782   Loss D 0.1173, Loss G: 0.8262, D_mean: 0.6294\n",
      "Epoch 84/100, Batch 781/782   Loss D 0.0494, Loss G: 0.8683, D_mean: 0.8402\n",
      "Epoch 85/100, Batch 781/782   Loss D 0.1374, Loss G: 0.7082, D_mean: 0.5706\n",
      "Epoch 86/100, Batch 781/782   Loss D 0.0604, Loss G: 0.7634, D_mean: 0.9973\n",
      "Epoch 87/100, Batch 781/782   Loss D 0.2046, Loss G: 0.5040, D_mean: 0.5775\n",
      "Epoch 88/100, Batch 781/782   Loss D 0.0281, Loss G: 0.9948, D_mean: 0.8853\n",
      "Epoch 89/100, Batch 781/782   Loss D 0.1781, Loss G: 0.3379, D_mean: 0.6263\n",
      "Epoch 90/100, Batch 781/782   Loss D 0.0739, Loss G: 0.7243, D_mean: 0.8461\n",
      "Epoch 91/100, Batch 781/782   Loss D 0.0455, Loss G: 0.8490, D_mean: 0.9973\n",
      "Epoch 92/100, Batch 781/782   Loss D 0.0599, Loss G: 0.7681, D_mean: 0.7189\n",
      "Epoch 93/100, Batch 781/782   Loss D 0.0710, Loss G: 0.8299, D_mean: 0.8846\n",
      "Epoch 94/100, Batch 781/782   Loss D 0.0573, Loss G: 0.9297, D_mean: 0.8989\n",
      "Epoch 95/100, Batch 781/782   Loss D 0.0337, Loss G: 0.8480, D_mean: 0.8431\n",
      "Epoch 96/100, Batch 781/782   Loss D 0.0372, Loss G: 0.8663, D_mean: 0.9362\n",
      "Epoch 97/100, Batch 781/782   Loss D 0.1110, Loss G: 0.7873, D_mean: 0.6108\n",
      "Epoch 98/100, Batch 781/782   Loss D 0.1949, Loss G: 0.7145, D_mean: 0.5001\n",
      "Epoch 99/100, Batch 781/782   Loss D 0.0755, Loss G: 0.6060, D_mean: 0.6736\n"
     ]
    }
   ],
   "source": [
    "print('Start training')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "    ### Discriminator training\n",
    "    data = data.to(device)\n",
    "    batch_size = data.shape[0]\n",
    "\n",
    "    net_D.zero_grad()\n",
    "    labels = (torch.ones(batch_size)*0.9).to(device)\n",
    "\n",
    "    output = net_D(data).reshape(-1)\n",
    "    loss_D_real = criterion(output, labels)\n",
    "\n",
    "    D_mean = output.mean().item()\n",
    "\n",
    "    noise = torch.randn(batch_size, channels_noise, 1, 1).to(device)\n",
    "    fake = net_G(noise) \n",
    "    labels = (torch.ones(batch_size)*0.1).to(device)\n",
    "\n",
    "    output = net_D(fake.detach()).reshape(-1)\n",
    "    loss_D_fake = criterion(output, labels)\n",
    "\n",
    "    loss_D = loss_D_real + loss_D_fake\n",
    "    loss_D.backward()\n",
    "\n",
    "    optimizator_D.step()\n",
    "    \n",
    "    ### Generator training\n",
    "    net_G.zero_grad()\n",
    "    labels = torch.ones(batch_size).to(device)\n",
    "\n",
    "    output = net_D(fake).reshape(-1)\n",
    "    loss_G = criterion(output, labels)\n",
    "    loss_G.backward()\n",
    "    optimizator_G.step()\n",
    "    \n",
    "  fake = net_G(fixed_noise)\n",
    "  vutils.save_image(fake.detach(),'output/fake_samples_epoch_%03d.png' %(epoch),normalize=True)\n",
    "    \n",
    "    \n",
    "  print(f'Epoch {epoch}/{num_epochs}, Batch {batch_idx}/{len(dataloader)} \\\n",
    "  Loss D {loss_D:.4f}, Loss G: {loss_G:.4f}, D_mean: {D_mean:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
